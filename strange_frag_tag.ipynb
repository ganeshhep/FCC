{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshhep/FCC/blob/main/strange_frag_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-If0IlfKkCl",
        "outputId": "b770c393-83ae-4522-f8a5-136486ce5465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastjet\n",
            "  Downloading fastjet-3.4.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting awkward>=2 (from fastjet)\n",
            "  Downloading awkward-2.7.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from fastjet) (1.26.4)\n",
            "Collecting vector (from fastjet)\n",
            "  Downloading vector-1.6.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting awkward-cpp==44 (from awkward>=2->fastjet)\n",
            "  Downloading awkward_cpp-44-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2->fastjet) (2024.10.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2->fastjet) (8.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from awkward>=2->fastjet) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2->fastjet) (3.21.0)\n",
            "Downloading fastjet-3.4.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward-2.7.4-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.4/871.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading awkward_cpp-44-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (638 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.7/638.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vector-1.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vector, awkward-cpp, awkward, fastjet\n",
            "Successfully installed awkward-2.7.4 awkward-cpp-44 fastjet-3.4.2.1 vector-1.6.1\n",
            "Collecting uproot\n",
            "  Downloading uproot-5.5.2-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: awkward>=2.4.6 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.7.4)\n",
            "Requirement already satisfied: cramjam>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from uproot) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from uproot) (2024.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from uproot) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from uproot) (24.2)\n",
            "Collecting xxhash (from uproot)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: awkward-cpp==44 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (44)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from awkward>=2.4.6->uproot) (8.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->awkward>=2.4.6->uproot) (3.21.0)\n",
            "Downloading uproot-5.5.2-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.5/363.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, uproot\n",
            "Successfully installed uproot-5.5.2 xxhash-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fastjet\n",
        "!pip install uproot\n",
        "!pip install torch\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXtEckgxLcyN"
      },
      "outputs": [],
      "source": [
        "import uproot\n",
        "import awkward as ak\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import itertools\n",
        "from fastjet import PseudoJet, JetDefinition, ClusterSequence, antikt_algorithm, sorted_by_pt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19lPnCxXLpbH",
        "outputId": "a7ca663c-351a-45b6-d26f-24102e72a6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_file_1 = '/content/drive/MyDrive/ML_HEP/Data_files/fcc_ee_h_ss.root'\n",
        "data_file_2 = '/content/drive/MyDrive/ML_HEP/Data_files/fcc_ee_h_qq.root'\n",
        "tree_1 = uproot.open(data_file_1)['events']\n",
        "tree_2 = uproot.open(data_file_2)['events']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzSq1llCj-JN"
      },
      "outputs": [],
      "source": [
        "pid = ak.concatenate([tree_1['Particle.PDG'].array(), tree_2['Particle.PDG'].array()], axis = 0)\n",
        "p_status = ak.concatenate([tree_1['Particle.generatorStatus'].array(), tree_2['Particle.generatorStatus'].array()], axis = 0)\n",
        "charge = ak.concatenate([tree_1['Particle.charge'].array(), tree_2['Particle.charge'].array()], axis = 0)\n",
        "m = ak.concatenate([tree_1['Particle.mass'].array(), tree_2['Particle.mass'].array()], axis = 0)\n",
        "px = ak.concatenate([tree_1['Particle.momentum.x'].array(), tree_2['Particle.momentum.x'].array()], axis = 0)\n",
        "py = ak.concatenate([tree_1['Particle.momentum.y'].array(), tree_2['Particle.momentum.y'].array()], axis = 0)\n",
        "pz = ak.concatenate([tree_1['Particle.momentum.z'].array(), tree_2['Particle.momentum.z'].array()], axis = 0)\n",
        "p_begin = ak.concatenate([tree_1['Particle.parents_begin'].array(), tree_2['Particle.parents_begin'].array()], axis = 0)\n",
        "p_end = ak.concatenate([tree_1['Particle.parents_end'].array(), tree_2['Particle.parents_end'].array()], axis = 0)\n",
        "p_ind = ak.concatenate([tree_1['Particle#0.index'].array(), tree_2['Particle#0.index'].array()], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY7kz7_Vm84R"
      },
      "outputs": [],
      "source": [
        "# Function to calculate energy\n",
        "def energy(m, px, py, pz):\n",
        "    E = np.sqrt( m**2 + px**2 + py**2 + pz**2)\n",
        "    return E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y19lz9z0nM0Q"
      },
      "outputs": [],
      "source": [
        "e = energy(m, px, py, pz) # Energy of particles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzBGBeGinUd-"
      },
      "outputs": [],
      "source": [
        "def get_parent_ids(i, particle_ids, parent_indices, parents_begin, parents_end) :\n",
        "    \"\"\"\n",
        "    Find parent ID and it's index in the particle IDs list.\n",
        "\n",
        "    Parameters:\n",
        "        i (integer): Index of the stable particle.\n",
        "        particle_ids (list): A list of IDs of particles.\n",
        "        parent_indices (list): A list of parent indices of particles.\n",
        "        parents_begin (list): A list of indices of the first parent of each particle.\n",
        "        parents_end (list): A list of indices of the last parent of each particle.\n",
        "\n",
        "    Returns:\n",
        "        parent_id (integer): Parent ID of the stable particle.\n",
        "        parent_indx (integer): Index of the parent particle.\n",
        "\n",
        "    \"\"\"\n",
        "    pb = parents_begin[i]\n",
        "    pe = parents_end[i]\n",
        "\n",
        "    if pb == pe :\n",
        "      return 0\n",
        "    else :\n",
        "      for id in range(pb, pe) :\n",
        "        parent_indx = parent_indices[id]\n",
        "      return parent_indx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DErZK03wnxVt"
      },
      "outputs": [],
      "source": [
        "def find_parent_and_daughter_ids(indices, particle_ids, all_particle_ids):\n",
        "    \"\"\"\n",
        "    Find parent IDs and corresponding daughter IDs for each repeated index.\n",
        "\n",
        "    Parameters:\n",
        "        indices (list): A list of parent indices of stable particles.\n",
        "        particle_ids (list): A list of IDs of stable particles.\n",
        "        all_particle_ids (list): A list of all particle IDs.\n",
        "\n",
        "    Returns:\n",
        "        list: [[parent index, parent ID, list of daughter IDs],...,....].\n",
        "    \"\"\"\n",
        "    # Count occurrences of each index\n",
        "    counts = Counter(indices)\n",
        "\n",
        "    # Identify repeated indices\n",
        "    repeated_indices = {index for index, count in counts.items() if count >= 2}\n",
        "\n",
        "    # Collect parent and daughter IDs for repeated indices\n",
        "    parent_daughters = []\n",
        "    for index in repeated_indices:\n",
        "        parent_id = all_particle_ids[index]  # Get the parent ID using the index\n",
        "        daughters = [particle_ids[i] for i in range(len(indices)) if indices[i] == index]\n",
        "        parent_daughters.append([index, parent_id, daughters])\n",
        "\n",
        "    return parent_daughters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5qm-gwFpe06"
      },
      "outputs": [],
      "source": [
        "def get_graph_list(R, batch_size = 5000) : # R is the radius parameter\n",
        "  graph_list = [] # List of graph datasets\n",
        "\n",
        "  N = 200000 # number of events\n",
        "\n",
        "  deta_mean = -0.0015069327248682111 # Mean of dijet eta distributions\n",
        "  dpt_mean = 35.78125547420067 # Mean of dijet pt distributions\n",
        "  dN_mean = 43.16249 # Mean of dijet N distributions\n",
        "\n",
        "  deta_std = 1.1429906944033923 # Standard deviation of dijet eta distributions\n",
        "  dpt_std = 17.15648916096553 # Standard deviation of dijet pt distributions\n",
        "  dN_std = 12.49922825617246 # Standard deviation of dijet N distributions\n",
        "\n",
        "  # Define Google Drive save path\n",
        "  drive_path = \"/content/drive/MyDrive/Graph_Datasets/\"\n",
        "  os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  for start in range(0, N, batch_size):\n",
        "    end = min(start + batch_size, N)\n",
        "    batch_graphs = []\n",
        "\n",
        "    for n in range(start, end) :\n",
        "      particles_list = [] # A list of all the particles in an event\n",
        "\n",
        "      constituent_indx = [] # A list of indices of constitents of the dijet\n",
        "\n",
        "      parent_indices = [] # A list of parent indices of constitents of the dijet\n",
        "\n",
        "      pt_norm = [] # A list of normalized transverse momentum of the constituents of the dijet\n",
        "\n",
        "      ident = [] # A list containing identity of the particles\n",
        "\n",
        "      node_ft = [] # Node feature matrix\n",
        "\n",
        "      edge_ft = [] # Edge attribute matrix\n",
        "\n",
        "      photon_energy = 0 # Total photon energy\n",
        "      muon_energy = 0 # Total muon energy\n",
        "      electron_energy = 0 # Total electron energy\n",
        "      Ks_energy = 0 # Total reconstructed Ks meson energy\n",
        "      lambda_energy = 0 # Total reconstructed lambda baryon energy\n",
        "      lambdabar_energy = 0 # Total reconstructed lambdabar baryon energy\n",
        "\n",
        "      sts = p_status[n] # Generator status of particles\n",
        "      p_x = px[n] # X-momentum of particles\n",
        "      p_y = py[n] # Y-momentum of particles\n",
        "      p_z = pz[n] # Z-momentum of particles\n",
        "      E = e[n] # Energy of particles\n",
        "      pcharge = charge[n] # Charge of particles\n",
        "      ids = pid[n] # PDG id of particles\n",
        "      pinds = p_ind[n] # Parent indices of particles\n",
        "      pbegin = p_begin[n] # Parent begin indices of particles\n",
        "      pend = p_end[n] # Parent end indices of particles\n",
        "\n",
        "      for i in range(len(p_x)) :\n",
        "        part_p4 = PseudoJet(float(p_x[i]), float(p_y[i]), float(p_z[i]), float(E[i]))\n",
        "        part_p4.set_user_index(i) # setting the index of the particle\n",
        "        particles_list.append(part_p4)\n",
        "\n",
        "      stable_p4 = [] # A list of stable particles in the event\n",
        "\n",
        "      for i in range(len(particles_list)) :\n",
        "        if sts[i] == 1 :\n",
        "          stable_p4.append(particles_list[i])\n",
        "\n",
        "      # Applying clustering of particles with anti-kt algorithm\n",
        "      cluster = ClusterSequence(stable_p4, JetDefinition(antikt_algorithm, R))\n",
        "      jet_set = sorted_by_pt(cluster.inclusive_jets())\n",
        "\n",
        "      if len(jet_set) > 1 :\n",
        "        # Dijet kinematics\n",
        "        dijet = jet_set[0] + jet_set[1]\n",
        "        d_m = dijet.m() # invariant mass\n",
        "        d_pt = dijet.pt() # transverse momentum\n",
        "        d_eta = dijet.eta() # pseudo rapidity\n",
        "        d_e = dijet.e() # energy\n",
        "\n",
        "        Dijet_constituents = jet_set[0].constituents() + jet_set[1].constituents() # Dijet constituents\n",
        "\n",
        "        N_constituents = len(Dijet_constituents) # Total number of constituents N in the dijet\n",
        "\n",
        "        for i in range(N_constituents) :\n",
        "          constituent_indx.append(Dijet_constituents[i].user_index())\n",
        "\n",
        "        constituent_ids = ids[constituent_indx]\n",
        "\n",
        "        for j in constituent_indx :\n",
        "          const_id = ids[j]\n",
        "\n",
        "          if const_id == 22 :\n",
        "            photon_energy += particles_list[j].e()\n",
        "          if const_id == 13 :\n",
        "            muon_energy += particles_list[j].e()\n",
        "          if const_id == 11 :\n",
        "            electron_energy += particles_list[j].e()\n",
        "\n",
        "          parent_indx = get_parent_ids(j, ids, pinds, pbegin, pend)\n",
        "          parent_indices.append(parent_indx)\n",
        "\n",
        "          parent_id = ids[parent_indx]\n",
        "\n",
        "          pt_norm.append(particles_list[j].pt()/d_pt) # Normalized pT\n",
        "\n",
        "          p_charge = pcharge[j] # Particle charge\n",
        "\n",
        "          if p_charge > 0 or parent_id == 3122 :\n",
        "            ident.append(1)\n",
        "          elif p_charge < 0 or parent_id == 310 :\n",
        "            ident.append(-1)\n",
        "          else :\n",
        "            ident.append(0)\n",
        "\n",
        "        pd = find_parent_and_daughter_ids(parent_indices, constituent_ids, ids)\n",
        "\n",
        "        mes_count = 0 # Strange meson count\n",
        "        bar_count = 0 # Strange baryon count\n",
        "\n",
        "        for k in range(len(pd)) :\n",
        "          pd_indx = pd[k][0]\n",
        "          pd_id = pd[k][1]\n",
        "\n",
        "          if pd_id == 310 :\n",
        "            mes_count = 1\n",
        "            Ks_energy += particles_list[pd_indx].e()\n",
        "          if pd_id == 3122 :\n",
        "            bar_count = 1\n",
        "            lambda_energy += particles_list[pd_indx].e()\n",
        "          if pd_id == -3122 :\n",
        "            bar_count = 1\n",
        "            lambdabar_energy += particles_list[pd_indx].e()\n",
        "\n",
        "        for l in range(len(constituent_indx)) :\n",
        "          node_ft.append([(d_eta - deta_mean)/deta_std, (d_pt - dpt_mean)/dpt_std, (N_constituents - dN_mean)/dN_std, photon_energy/d_e, muon_energy/d_e, electron_energy/d_e, Ks_energy/d_e, lambda_energy/d_e, lambdabar_energy/d_e, pt_norm[l], ident[l]])\n",
        "\n",
        "        x = torch.tensor(node_ft, dtype = torch.float).to(device) # Node features matrix\n",
        "\n",
        "        y = torch.tensor([mes_count, bar_count]).to(device) # Target features\n",
        "\n",
        "        edge_index = torch.tensor(list(itertools.permutations(range(N_constituents), 2))).T.to(device) # Edge index\n",
        "\n",
        "        src = edge_index[0].tolist()\n",
        "        dst = edge_index[1].tolist()\n",
        "\n",
        "        for i in range(len(src)) :\n",
        "          node_i = src[i]\n",
        "          node_j = dst[i]\n",
        "\n",
        "          edge_ft.append([(d_eta - deta_mean)/deta_std, (d_pt - dpt_mean)/dpt_std, (N_constituents - dN_mean)/dN_std, photon_energy/d_e, muon_energy/d_e, electron_energy/d_e, Ks_energy/d_e, lambda_energy/d_e, lambdabar_energy/d_e, pt_norm[node_i], ident[node_i], pt_norm[node_j], ident[node_j]])\n",
        "\n",
        "        edge_attr = torch.tensor(edge_ft, dtype = torch.float).to(device) # Edge attributes matrix\n",
        "\n",
        "        graph = Data(x = x, edge_index = edge_index, edge_attr = edge_attr, y = y) # Graph data\n",
        "\n",
        "        batch_graphs.append(graph)\n",
        "\n",
        "    # Save batch graphs to Google Drive\n",
        "    save_path = os.path.join(drive_path, f\"graph_batch_{start}_{end}.pt\")\n",
        "    torch.save(batch_graphs, save_path)\n",
        "    print(f\"Saved batch: {save_path}\")\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7aewRj0fDWx",
        "outputId": "6c6096f7-217d-42e5-cafc-28c5e1eb93bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_0_5000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_5000_10000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_10000_15000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_15000_20000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_20000_25000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_25000_30000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_30000_35000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_35000_40000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_40000_45000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_45000_50000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_50000_55000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_55000_60000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_60000_65000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_65000_70000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_70000_75000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_75000_80000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_80000_85000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_85000_90000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_90000_95000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_95000_100000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_100000_105000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_105000_110000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_110000_115000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_115000_120000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_120000_125000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_125000_130000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_130000_135000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_135000_140000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_140000_145000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_145000_150000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_150000_155000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_155000_160000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_160000_165000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_165000_170000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_170000_175000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_175000_180000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_180000_185000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_185000_190000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_190000_195000.pt\n",
            "Saved batch: /content/drive/MyDrive/Graph_Datasets/graph_batch_195000_200000.pt\n"
          ]
        }
      ],
      "source": [
        "graph_list = get_graph_list(R = 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN1m7LYSCChw"
      },
      "outputs": [],
      "source": [
        "def load_graph_batches(drive_path = \"/content/drive/MyDrive/Graph_Datasets/\"):\n",
        "    graph_list = []\n",
        "\n",
        "    # Get all batch files\n",
        "    batch_files = sorted([f for f in os.listdir(drive_path) if f.endswith(\".pt\")])\n",
        "\n",
        "    for file in batch_files:\n",
        "        file_path = os.path.join(drive_path, file)\n",
        "        batch_graphs = torch.load(file_path)  # Load the batch\n",
        "        graph_list.extend(batch_graphs)  # Add to full list\n",
        "\n",
        "    return graph_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGYw6KPzCZEX",
        "outputId": "300b39af-7e7c-4a0a-ad13-eba54d68f3a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-373a04f493d8>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  batch_graphs = torch.load(file_path)  # Load the batch\n"
          ]
        }
      ],
      "source": [
        "graph_list = load_graph_batches()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqYXOAylF2L9",
        "outputId": "db859b69-e013-47c8-de3b-76400b3f533e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(graph_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aAKjpmkKtZU",
        "outputId": "bc337041-0af4-4847-b0d6-c07ba7d0908f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 140000 graphs (Balanced: 70000 signal, 70000 background)\n",
            "Validation set: 30000 graphs (Balanced: 15000 signal, 15000 background)\n",
            "Test set: 30000 graphs (Balanced: 15000 signal, 15000 background)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate signal and background\n",
        "signal_graphs = graph_list[:100000]  # First 100K are H → s s̄\n",
        "background_graphs = graph_list[100000:]  # Next 100K are H → q q̄\n",
        "\n",
        "# Train-validation-test split (70-15-15)\n",
        "train_signal, temp_signal = train_test_split(signal_graphs, test_size = 0.30, random_state = 42)\n",
        "train_background, temp_background = train_test_split(background_graphs, test_size = 0.30, random_state = 42)\n",
        "\n",
        "val_signal, test_signal = train_test_split(temp_signal, test_size = 0.50, random_state = 42)\n",
        "val_background, test_background = train_test_split(temp_background, test_size = 0.50, random_state = 42)\n",
        "\n",
        "# Merge signal and background in each split\n",
        "train_data = train_signal + train_background\n",
        "val_data = val_signal + val_background\n",
        "test_data = test_signal + test_background\n",
        "\n",
        "# DataLoader for each split\n",
        "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
        "val_loader = DataLoader(val_data, batch_size = 64, shuffle = False)\n",
        "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)\n",
        "\n",
        "print(f\"Training set: {len(train_data)} graphs (Balanced: {len(train_signal)} signal, {len(train_background)} background)\")\n",
        "print(f\"Validation set: {len(val_data)} graphs (Balanced: {len(val_signal)} signal, {len(val_background)} background)\")\n",
        "print(f\"Test set: {len(test_data)} graphs (Balanced: {len(test_signal)} signal, {len(test_background)} background)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I_HiKdKnScfi"
      },
      "outputs": [],
      "source": [
        "class MPNN(MessagePassing) :\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__(aggr = 'mean')\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "        self.edge_lin = torch.nn.Linear(13, out_channels)\n",
        "        self.out_layer = torch.nn.Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr) :\n",
        "        x = self.lin(x)\n",
        "        return self.propagate(edge_index, x = x, edge_attr = edge_attr)\n",
        "\n",
        "    def message(self, x_j, edge_attr) :\n",
        "        return x_j + self.edge_lin(edge_attr)\n",
        "\n",
        "    def update(self, aggr_out) :\n",
        "        return self.out_layer(aggr_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t7abvDZ8TF5K"
      },
      "outputs": [],
      "source": [
        "class GraphMPNN(torch.nn.Module) :\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels = 2) :  # 2 output neurons for classification\n",
        "        super().__init__()\n",
        "        self.mpnn = MPNN(in_channels, hidden_channels)\n",
        "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data) :\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        x = self.mpnn(x, edge_index, edge_attr)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.fc(x)  # No softmax, since BCEWithLogitsLoss expects raw logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJua_YRGUe2F",
        "outputId": "63e3c8ed-f9b1-44cb-a318-75fa5bb27ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0057, Validation Accuracy: 99.71%\n",
            "Epoch 2, Loss: 0.0064, Validation Accuracy: 99.26%\n",
            "Epoch 3, Loss: 0.0020, Validation Accuracy: 99.88%\n",
            "Epoch 4, Loss: 0.0032, Validation Accuracy: 99.88%\n",
            "Epoch 5, Loss: 0.0006, Validation Accuracy: 99.82%\n",
            "Epoch 6, Loss: 0.0009, Validation Accuracy: 99.95%\n",
            "Epoch 7, Loss: 0.0001, Validation Accuracy: 99.90%\n",
            "Epoch 8, Loss: 0.0006, Validation Accuracy: 99.98%\n",
            "Epoch 9, Loss: 0.0002, Validation Accuracy: 99.87%\n",
            "Epoch 10, Loss: 0.0002, Validation Accuracy: 99.97%\n",
            "Epoch 11, Loss: 0.0001, Validation Accuracy: 99.98%\n",
            "Epoch 12, Loss: 0.0005, Validation Accuracy: 99.93%\n",
            "Epoch 13, Loss: 0.0000, Validation Accuracy: 99.99%\n",
            "Epoch 14, Loss: 0.0003, Validation Accuracy: 99.98%\n",
            "Epoch 15, Loss: 0.0001, Validation Accuracy: 99.99%\n",
            "Epoch 16, Loss: 0.0000, Validation Accuracy: 99.94%\n",
            "Epoch 17, Loss: 0.0003, Validation Accuracy: 99.93%\n",
            "Epoch 18, Loss: 0.0002, Validation Accuracy: 100.00%\n",
            "Epoch 19, Loss: 0.0000, Validation Accuracy: 99.95%\n",
            "Epoch 20, Loss: 0.0347, Validation Accuracy: 99.96%\n",
            "Epoch 21, Loss: 0.0050, Validation Accuracy: 99.97%\n",
            "Epoch 22, Loss: 0.0000, Validation Accuracy: 99.76%\n",
            "Epoch 23, Loss: 0.0006, Validation Accuracy: 99.98%\n",
            "Epoch 24, Loss: 0.0001, Validation Accuracy: 99.89%\n",
            "Epoch 25, Loss: 0.0000, Validation Accuracy: 99.93%\n",
            "Epoch 26, Loss: 0.0001, Validation Accuracy: 99.98%\n",
            "Epoch 27, Loss: 0.0005, Validation Accuracy: 99.94%\n",
            "Epoch 28, Loss: 0.0006, Validation Accuracy: 99.99%\n",
            "Epoch 29, Loss: 0.0001, Validation Accuracy: 99.99%\n",
            "Epoch 30, Loss: 0.0000, Validation Accuracy: 99.95%\n",
            "Epoch 31, Loss: 0.0005, Validation Accuracy: 100.00%\n",
            "Epoch 32, Loss: 0.0000, Validation Accuracy: 99.91%\n",
            "Epoch 33, Loss: 0.0000, Validation Accuracy: 100.00%\n",
            "Epoch 34, Loss: 0.0000, Validation Accuracy: 100.00%\n",
            "Epoch 35, Loss: 0.0011, Validation Accuracy: 99.97%\n",
            "Epoch 36, Loss: 0.0002, Validation Accuracy: 99.99%\n",
            "Epoch 37, Loss: 0.0000, Validation Accuracy: 100.00%\n",
            "Epoch 38, Loss: 0.0000, Validation Accuracy: 99.83%\n",
            "Epoch 39, Loss: 0.0086, Validation Accuracy: 99.96%\n",
            "Epoch 40, Loss: 0.0002, Validation Accuracy: 99.94%\n",
            "Epoch 41, Loss: 0.0000, Validation Accuracy: 100.00%\n",
            "Epoch 42, Loss: 0.0009, Validation Accuracy: 99.97%\n",
            "Epoch 43, Loss: 0.0000, Validation Accuracy: 99.92%\n",
            "Epoch 44, Loss: 0.0000, Validation Accuracy: 99.89%\n",
            "Epoch 45, Loss: 0.0000, Validation Accuracy: 99.89%\n",
            "Epoch 46, Loss: 0.0001, Validation Accuracy: 99.97%\n",
            "Epoch 47, Loss: 0.0000, Validation Accuracy: 99.96%\n",
            "Epoch 48, Loss: 0.0004, Validation Accuracy: 99.98%\n",
            "Epoch 49, Loss: 0.0000, Validation Accuracy: 100.00%\n",
            "Epoch 50, Loss: 0.0002, Validation Accuracy: 99.79%\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define model\n",
        "model = GraphMPNN(in_channels = 11, hidden_channels = 16, out_channels = 2).to(device)\n",
        "\n",
        "# Optimizer and Binary Cross-Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()  # Multi-label classification loss\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # No gradients needed for evaluation\n",
        "        for batch in data_loader:\n",
        "            out = model(batch)\n",
        "            probs = torch.sigmoid(out)  # Convert logits to probabilities\n",
        "            preds = (probs > 0.5).int()  # Convert probabilities to binary labels (0 or 1)\n",
        "            correct += (preds == batch.y.view(-1, 2)).sum().item()\n",
        "            total += batch.y.numel()  # Count total number of labels (in the batch)\n",
        "    return correct / total\n",
        "\n",
        "# Training loop with validation\n",
        "best_val_acc = 0\n",
        "for epoch in range(50):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y.view(-1, 2).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on validation set after each epoch\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Validation Accuracy: {val_acc * 100:.2f}%\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "# Load the best model for final testing\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", weights_only = True))\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_acc = evaluate(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyP94OahtJAG+qUSTcq0Fgm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}